# network namespace

容器使用namespace做资源隔离, 其中network namespace来做网络资源隔离, 可以使用 `ip netns list`来查看当前network namespace

对应的network namespace操作命令已经集成到Linux ip命令的子命令 `ip netns`中:

```
ip netns add netns1  #创建名为netns1的network namespace
ip netns exec netns1  #进入netns1
ip netns exec netns1 ip link list  #进入netns1执行ip link list
ip netns delete netns1  #删除netns1
```

## 查看不同进程是否在同一ns

每个linux进程都有一个 `/proc/{PID}/ns`目录, 这里面每个文件都代表一个类型的namespace

```
[root@localhost ns]# ls -l
total 0
lrwxrwxrwx 1 root root 0 Jan 27 12:22 ipc -> ipc:[4026531839]
lrwxrwxrwx 1 root root 0 Jan 27 12:22 mnt -> mnt:[4026531840]
lrwxrwxrwx 1 root root 0 Jan 27 12:22 net -> net:[4026531956]
lrwxrwxrwx 1 root root 0 Jan 27 12:22 pid -> pid:[4026531836]
lrwxrwxrwx 1 root root 0 Jan 27 12:22 user -> user:[4026531837]
lrwxrwxrwx 1 root root 0 Jan 27 12:22 uts -> uts:[4026531838]
```

如果两个进程属于同一个ns, 对应ns后面的inode数字(即[4026531956])相同

## ns的绑定和解绑

通过Linux系统的系统调用 `setns()` 把一个进程计入一个已经存在的 ns中

通过系统调用 `unshare()` 把进程移出ns中

# veth pair

veth是虚拟以太网卡的缩写(virtual Ethernet)的缩写, 总是成对出现, 所以称为veth pair, 常用于跨namespace之间通信

## 创建

创建veth pair, 名字分别是veth0和veth1

```
ip link add veth0 type veth peer name veth1
```

## 查看

```
ip link list

5: veth1@veth0: <BROADCAST,MULTICAST,M-DOWN> mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000
    link/ether 12:28:7c:43:8f:e6 brd ff:ff:ff:ff:ff:ff
6: veth0@veth1: <BROADCAST,MULTICAST,M-DOWN> mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000
    link/ether 62:0f:93:c0:86:8d brd ff:ff:ff:ff:ff:ff
```

初始创建的veth pair表现为2块网卡, 初始状态是down, 可以通过`ip link`设置为up

```
ip link set dev veth0 up
```

veth pair同样可以配置IP地址

```
ifconfig veth0 10.20.30.40/24
```

将veth pair放入namespace中

```
ip link set veth0 netns newns
```

## 查看容器和宿主机对应的veth pair

如果容器内没有安装对应的工具, 简单方式可以**进入容器内**查看iflink的序号, 可以看到是11

```
root@3f555d52d6eb:/# cat /sys/class/net/eth0/iflink
11
``` 

然后再宿主机上, 查看到11的veth, 这两个就是一对

```
[root@localhost ~]# ip link show

......
11: vethd71cf35@if10: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue master docker0 state UP mode DEFAULT group default
    link/ether 12:a6:06:45:2a:20 brd ff:ff:ff:ff:ff:ff link-netnsid 0
```

注意, `vethd71cf35@if10` 这种格式的名称, 后面的 `@if10`, 其中10就是和它成对的veth pair的index

如果镜像中有网络工具, 可以在容器中使用 `ip link show eth0` 或者 `ethtool -S`来显示

# tun/tap设备

什么是 tun/tap设备, 从网络虚拟化的角度来说, 它是虚拟网卡, 一端连着网络协议栈, 一端连着用户态程序.

但是他们还是有些区别的, tun表示虚拟的是点对点设备, tap表示虚拟的以太网设备, 这两种设备针对的网络包实施不同的封装

tun/tap设备可以将TCP/IP协议栈处理好的网络包发给任何一个使用tun/tap驱动的进程, 由进程重新处理后发到物理链路中. tun/tap设备就像埋在用户程序空间的一个钩子, 我们可以很方便的将对网络包的处理程序挂在这个钩子上

从网络协议栈的角度看, tun/tap设备这类虚拟网卡和物理网卡没有区别, 只是对tun/tap设备而言, 它与物理网卡的不同之处在于**它的数据源不是物理链路, 而是来自于用户态!**, 所哦以tun/tap设备就是利用Linux的设备文件实现内核态和用户态数据交换

普通物理网卡是通过网线收发数据包, 而tun设备通过一个设备文件(/dev/tunX)收发数据包, 所有对这个文件的写操作都会通过tun设备转换成一个数据包发给内核网络协议栈, 当内核发一个包给tun设备的时候, 用户态进程可以读取这个文件拿到包内容.

tun和tap设备区别:

- tun设备的 /dev/tunX 文件收发的是IP包, 因此只能工作在L3, 无法和物理网卡做桥接, 但可以通过三层交换,(例如 ip_forward)与物理网卡连通;
- tap设备的/dev/tapX 文件收发的是链路层数据包, 可以与物理网卡桥接





